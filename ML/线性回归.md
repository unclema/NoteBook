# 什么是线性回归

回归是指预测连续的实数值，而线性回归基本就可以理解为在数据中画出一根线来进行预测值即**通过线性方程去拟合数据点**

# 线性回归可以干什么

可以实现对连续值得预测，例如股票得价格、房价得趋势等

# 线性回归是怎么推导的

## 一元线性回归

求解一个函数使之可以拟合数据点，一元线性回归可以使它的函数表达式为
$$
y(x, w) = w_0 + w_1x \tag{1}
$$
而我们要求得就是公式$$(1)$$中得$$w_0$$和$$w_1$$

而求的得方式就是找到一条线，这条线到每个散点的距离（蓝线）和最小，这就牵扯到另一个概念**平方损失函数**

![](https://gitee.com/unclema/picture_bed/raw/master/20200628095110.png)

## 平方损失函数

我们设一个数据点的坐标为$$(x_i,y_i)$$，那么直线到这个点的距离(误差)就为
$$
y_i-(w_0+w_1x_i) \tag{2}
$$
上面的误差也被称为【残值】，在机器学习中称为【损失】，即真实值和预测值之间的偏离程度。那么对于$$n$$个全部数据点而言，总和就为
$$
\sum\limits_{i=1}^n(y_{i} - (w_0 + w_1x_{i})) \tag{3}
$$
为了避免正负残值抵消，我们一般给他来一个平方
$$
\sum\limits_{i=1}^n{(y_i-(w_0+w_1x_i))^2}\tag{4}
$$
这个$$(4)$$就被称为**平方损失函数**，我们的目标就是让它的值最小。

如何让他最小，就用到了**最小二乘法**

## 最小二乘法

最小二乘法就是求平方损失函数最小的方法，这里有两种方法可以进行推导，一种是微积分的方法，一种是线性代数的方法。

### 代数求解

总体思路就是二元函数求极值

已知平方损失函数为：
$$
f = \sum\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}^2} \tag{5}
$$
分别对$w_{0}$和$w_{1}$求偏导，当$\frac{\partial f} {\partial w_{0}} = 0$ 和$\frac {\partial f} {\partial w_{1}} = 0$时，本函数取得最值
$$
\frac{\partial f}{\partial w_{0}} = -2({\sum_{i = 1}^{n} {y_i} - nw_{0}-w_{1}\sum_{i=1}^{n}{x_i}) \tag{6a}}
$$

$$
\frac {\partial f} {\partial w_{1}}=-{2(\sum_{i=1}^{n}x_i y_i - w_{0}\sum_{i=1}^{n}x_i - w_1 \sum_{i=1}^{n}x_{i}^2)} \tag{6b}
$$

然后，我们令 $\frac{\partial f}{\partial w_{0}}=0$ 以及  $\frac{\partial f}{\partial w_{1}}=0$，解得：
$$
w_{1}=\frac {n\sum_{}^{}{x_iy_i}-\sum_{}^{}{x_i}\sum_{}^{}{y_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2} \tag{7b}
$$

$$
w_{0}=\frac {\sum_{}^{}{x_i}^2\sum_{}^{}{y_i}-\sum_{}^{}{x_i}\sum_{}^{}{x_iy_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2} \tag{7b}
$$

此时就求得了$w_{1} 和w_{0}$的值也就求得了最佳拟合直线

### 矩阵求解

我们可以将函数$ y(x, w) = w_0 + w_1x$写成矩阵
$$
\left[ \begin{array}{c}
{1, x_{1}} \\ {1, x_{2}} \\ {\cdots} \\ {1, x_{9}} \\ {1, x_{10}}
\end{array}\right]
\left[ \begin {array}{c}
{w_{0}}\\{w_{1}}
\end{array}\right]
=
\left[\begin{array}{c}
{y_1}\\{y_2}\\{\cdots}\\{y_9}\\{y_10}
\end{array}\right] \tag{8a}
$$
即
$$
y(x,w) = XW \tag{8b}
$$


$(8)$ 式中，$W$ 为 $\begin{bmatrix}w_{0}
\\ w_{1}
\end{bmatrix}$，而 $X$ 则是 $\begin{bmatrix}1, x_{1}
\\ 1, x_{2}
\\ \cdots
\\ 1, x_{9}
\\ 1, x_{10}
\end{bmatrix}$ 矩阵。然后，平方损失函数$f=\sum\limits_{i=1}^n(y_i -(w_0 +w_1w_i))^2$可以写为
$$
(y-XW)^T(y-XW)   \tag{9}
$$


我们可以对上式使用乘法分配律
$$
f=y^Ty - y^T(XW)-(XW)^Ty+(XW)^T(XW) \tag{10}
$$
通过看$(8a)$可以知道$y和XW$都是m行1列的矩阵，所以他们相乘效果一样
$$
f = y^{T}y - (XW)^{T}y - (XW)^{T}y + (XW)^{T}(XW)\\ = y^{T}y - 2 (XW)^{T}y + (XW)^{T}(XW) \tag{11}
$$
对$(11)$进行求偏导
$$
\frac{\partial f}{\partial W}=2X^TXW-2X^Ty=0 \tag{12}
$$
当矩阵 $X^TX$ 满秩时， $(X^TX)^{-1}X^TX=E$，且 $EW=W$。所以有 $(X^TX)^{-1}X^TXW=(X^TX)^{-1}X^Ty$，并最终得到：
$$
W=(X^TX)^{-1}X^Ty \tag{13}
$$



# 线性回归的使用

下面是怎样用代码对函数进行实现

## 不使用库

### 线性回归函数的表达

$$ y(x, w) = w_0 + w_1x $$

```python
def f(x, w0, w1):
    y = w0 + w1 * x
    return y
```

### 平方损失函数的表达

$$
\sum\limits_{i = 1}^n {{{(y_{i}-(w_0 + w_1x_{i}))}}^2}
$$

```python
def square_loss(x, y, w0, w1):
    loss = sum(np.square(y - (w0 + w1 * x)))
    return loss
```

### 线性回归表达

#### 使用代数求解

$$
w_{1}=\frac {n\sum_{}^{}{x_iy_i}-\sum_{}^{}{x_i}\sum_{}^{}{y_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}
$$

$$
w_{0}=\frac {\sum_{}^{}{x_i}^2\sum_{}^{}{y_i}-\sum_{}^{}{x_i}\sum_{}^{}{x_iy_i}}  {n\sum_{}^{}{x_i}^2-(\sum_{}^{}{x_i})^2}
$$

```python
def w_calculator(x, y):
    n = len(x)
    w1 = (n*sum(x*y) - sum(x)*sum(y))/(n*sum(x*x) - sum(x)*sum(x))
    w0 = (sum(x*x)*sum(y) - sum(x)*sum(x*y))/(n*sum(x*x)-sum(x)*sum(x))
    return w0, w1
```

#### 矩阵求解

$$
W=(X^TX)^{-1}X^Ty
$$

```python
def w_matrix(x, y):
    w = (x.T * x).I * x.T * y
    return w
```

### 使用步骤

以房价为例，x为面积，y为房价

```python
import numpy as np

x = np.array([56, 72, 69, 88, 102, 86, 76, 79, 94, 74])
y = np.array([92, 102, 86, 110, 130, 99, 96, 102, 105, 92])
```

求出$w_0和w_1$

```python
def w_calculator(x, y):
    n = len(x)
    w1 = (n*sum(x*y) - sum(x)*sum(y))/(n*sum(x*x) - sum(x)*sum(x))
    w0 = (sum(x*x)*sum(y) - sum(x)*sum(x*y))/(n*sum(x*x)-sum(x)*sum(x))
    return w0, w1
w_calculator(x,y)
```

好了，这就完成了一元函数的求解，得到了一条直线$$ y(x, w) = w_0 + w_1x $$此时想求得话只要把x,w0,w1带入即可

预测

```python
def f(x, w0, w1):
    y = w0 + w1 * x
    return y
f(150, w0, w1)
```



## 线性回归 scikit-learn 实现

使用 scikit-learn 实现线性回归的过程会简单很多，这里要用到 `LinearRegression()` 类 。看一下其中的参数：

```
sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=1)
```

- fit_intercept: 默认为 True，计算截距项。
- normalize: 默认为 False，不针对数据进行标准化处理。
- copy_X: 默认为 True，即使用数据的副本进行操作，防止影响原数据。
- n_jobs: 计算时的作业数量。默认为 1，若为 -1 则使用全部 CPU 参与运算。

```python
from sklearn.linear_model import LinearRegression

# 定义线性回归模型
model = LinearRegression()
model.fit(x.reshape(len(x), 1), y)  # 训练, reshape 操作把数据处理成 fit 能接受的形状

# 得到模型拟合参数
model.intercept_, model.coef_
```

通过 `model.intercept_` 可以得到拟合的截距项，即上面的 $w_{0}$，通过 `model.coef_` 得到 $x$ 的系数，即上面的 $w_{1}$

预测

```python
model.predict([[150]])
```

# 误差方法

构建出数学模型并求得一个数后，我们还需要对结果进行误差分析，一般误差分析有如下几种

## 平均绝对误差（MAE）

将真实值和预测值之间的差值取绝对值并求和，最后取一个平均值，这个值越小拟合度越好，数学公式可以表达为
$$
MAE(y,\hat{y}) = \frac {1}{n} \sum \limits_{i=1}^n|y_i - \hat{y_i}|
$$

```python
def mae_value(y_true, y_pred):
    n = len(y_true)
    mae = sum(np.abs(y_true - y_pred)) / n
    return mae
```

## 均方误差（MSE）

将真实值和预测值之间的差值取平方求和，最后取一个平均值，这个值越小拟合度越好，数学公式可以表达为
$$
MSE(y,\hat{y}) = \frac {1}{n} \sum\limits_{i=1}^n(y_i-\hat{y_i})^2
$$

```python
def mes_value(y_true, y_pred):
    n = len(y_true)
    mes = sum(np.square(y_true - y_pred)) / n
    return mse
```

## 平均绝对百分比误差 (MAPE)

MAPE 是一个百分比值，因此比其他统计量更容易理解。它是用真实值减去预测值除以预测值将其取绝对值求和再除以真实得个数乘以100，数学表达式如下
$$
MAPE(y,\hat{y})=\frac{1}{n} \sum\limits_{i=1}^n|\frac{y_i - \hat{y_i}}{y_i}|\times100
$$

```python
def mape(y_true,y_pred):
    n = len(y_true)
    mape = sum(np.abs((y_true - y_pred) / y_true)) / n * 100
    return mape
```

# 实例演示

预测北京房价,使用开源数据集https://github.com/PENGZhaoqing/scrapy-HousePricing

## 读取数据集

```python
import pands as pd
df= pd.read_csv("beijing.csv")
# 查看
df.head()
```

## 划分数据集

使用特征公交，写字楼，医院，商场，地铁，学校，建造时间，楼层，面积预测每平价格

```python
features = df[['公交','写字楼','医院','商场','地铁','学校','建造时间','楼层','面积']]
target = df['每平米价格']
```

划分训练集和特征集

```python
split_num = int(len(df)*0.7) # 70% 分割数
X_train = features[:split_num]
y_train = target[:split_num]
X_test = features[split_num:]
y_test = target[split_num:]
```

## 建立模型

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()#建立模型
model.fit(X_train,y_train)#训练模型
```

## 检验模型

```python
def mape(y_true,y_pred):
    n = len(y_true)
    mape = sum(np.abs((y_true - y_pred) / y_true)) / n * 100
    return mape
y_true = y_test.values
y_pred = model.predict(X_test)
mape(y_true, y_pred)
```



