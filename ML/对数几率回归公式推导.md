对数几率回归时解决二分类问题的，所谓的二分类就是输入一个数据，返回不是1就是零，最理想的函数时单位跃阶函数，但是因为他不连续，所以就要使用对数几率函数来进行替代。

对数几率函数为$y=\frac{1}{1+e^{-z}}$

# 一、广义线性模型

## 1.1 指数族分布

指数族分布式一类分布的总称，该类分布的分布律（或者概率密度函数）的一般形式如下：

$p(y;\eta) = b(y)exp(\eta^TT(y)-a(\eta))$其中，$\eta$称为该分布的自然参数；$T(y)$为充分统计量，视具体的分布而定，通常是等于随机变量y本身；$a(\eta)$为配分函数；$b(y)$为关于随机变量y的函数，常见的伯努利分布和正态分布均属于指数族分布。

**证明伯努利分布属于指数族分布：**

已知伯努利分布的分布律为：$p(y) = \phi ^y(1-\phi)^{1-y}其中$$y\in\{0,1\}$,$\phi表示y取到1的概率（1-\phi)为取到0的概率$ 对上式恒等变形可得
$$
\begin{aligned}
p(y) &= \phi ^y (1-\phi)^{1-y}
\\
&=e^{ln(\phi ^y (1-\phi)^{1-y})}
\\
&=e^{(ln(\phi ^y)+ln (1-\phi)^{1-y})}
\\
&=e^{(yln(\phi)+(1-y)ln (1-\phi))}
\\
&=e^{(yln(\phi)+ln (1-\phi)-yln (1-\phi))}
\\
&=e^{(y(ln(\phi)-ln (1-\phi))+ln (1-\phi))}
\\
&=e^{(yln\frac{\phi}{1-\phi}+ln (1-\phi))}
\\
对比指数族分布的一般形式 p(y;\eta) &= b(y)exp(\eta^TT(y)-a(\eta))可知：\\
b(y) = 1\ \ T(y)=y \ \ \eta &= ln\frac{\phi}{1-\phi}\ \ a(\eta) = -ln (1-\phi) = ln(1+e^\eta)
\end{aligned}
$$


## 1.2 广义线性模型的三条假设

1、在给定$x$的条件下，假设随机变量$y$服从某个指数族分布

2、在给定$x$的条件下，我们的目标是得到一个模型$h(x)$能预测出$T(y)$的期望值；

3、假设该指数族分布中的自然参数$\eta$和$x$呈线性关系，即$\eta = w^Tx$

符合这三条的都可以成为广义线性模型

# 二、对数几率回归

## 2.1 对数几率回归的广义线性模型推导

对数几率回归时在对一个二分类问题进行建模，并且假设被建模的随机变量y取值为0或1，因此我们很自然得假设y服从伯努利分布。此时如果我们想要构建一个线性模型来预测在给定x的条件下y的取值的话，可以考虑使用广义线性模型来进行建模。

已知y是服从伯努利分布，而伯努利分布属于指数族分布，所以满足广义线性模型的第一条假设，接着根据广义线性模型的第二条假设我们可以推得模型h(x)的表达式为：$h(x)=E[T(y|x)]$

由于伯努利分布的$T(y|x) = y|x$所以：$h(x) = E[y|x]$

又因为$E[y|x]=1 \times p(y=1|x) + 0 \times p(y=0|x) = p(y=1|x) = \phi$ 所以$h(x) = \phi$（满足条件二）

在前面证明伯努利分布属于指数族分布时我们知道：
$$
\begin{aligned}
\eta &= ln\frac{\phi}{1-\phi}
\\
e^\eta &= \frac{\phi}{1- \phi}
\\
e^{-\eta} &= \frac{1- \phi}{\phi}
\\
e^{-\eta} &= \frac{1}{\phi} - 1
\\
e^{-\eta}+1 &= \frac{1}{\phi}
\\
\frac{1}{e^{-\eta}+1} &= {\phi}
\end{aligned}
$$
将$\phi = \frac{1}{e^{-\eta}+1}$带入$h(x)$表达式可得：
$$
h(x)=\phi=\frac{1}{1+e^{-\eta}}
$$
根据广义模型第三条假设$\eta = w^Tx ， h{x}$最终可化为$h(x)=\phi=\frac{1}{1+e^{-w^Tx}}=p(y=1|x)$此即为对数几率回归

## 2.2极大似然估计法

设总体的概率密度函数（或分布律）为$f(y,w_1,w_2,\cdots,w_k),y_1,y_2,\cdots,y_m$为从该总体中抽出的样本。因为$y_1,y_2,\cdots,y_m$相互独立且同分布，于是，它们的联合概率密度函数（或联合概率）为：
$$
L(y_1,y_2,\cdots,y_m;w_1,w_2,\cdots,w_k)= \prod_{i=1}^mf(y_i,w_1,w_2,\cdots,w_k)
$$
其中，$w_1,w_2,\cdots,w_k$被看作固定但是未知的参数。当我们已经观测到一组样本观测值$y_1,y_2,\cdots,y_m$时，要去估计未知参数，一种直观的想法就是，哪一组参数值使得现在得样本观测值出现的概率最大，哪一组参数可能就是真正得参数，我们就用它作为参数得估计值，这就是所谓得极大似然估计。

极大似然估计得具体方法：

通常记$L(y_1,y_2,\cdots，y_m;w_1,w_2,\cdots,w_k)=L(w)$,并称其为似然函数。于是求$w$得极大似然估计就归结为求$L(w)$的最大值点。由于对数函数使单调递增函数，所以
$$
lnL(w)= ln(\prod_{i=1}^mf(y_i,w_1,w_2,\cdots,w_k)) = \sum_{i=1}^mlnf(y_i,w_1,w_2,\cdots,w_k)
$$
与$L(w)$有相同的最大值点，而在许多情况下，求$lnL(w)$的最大值点比较简单，于是，我们就将求L(w)的最大值点转化为了求$lnL(w)$的最大值点，通常称$lnL(w)$为对数似然函数。

## 2.3 对数几率回归的参数估计

已知随机变量y取1和0的概率分别为
$$
\begin{aligned}
p(y=1|x) &=\frac{e^{w^Tx +b}}{1+e^{w^Tx +b}}
\\
p(y=0|x) &= \frac{1}{1+e^{w^Tx +b}}
\\
\beta &= (w;b) \ \ \ \ \  \hat x=(x;1)
\\
w^Tx +b &= \beta^Tx
\\
p(y=1|x) &=\frac{e^{\beta^Tx}}{1+e^{\beta^Tx}} = p_1(\hat x;\beta)
\\
p(y=0|x) &= \frac{1}{1+e^{\beta^Tx}}=p_0(\hat x;\beta)
\\
\end{aligned}
$$
于是，可以得到随机变量y的分布律表达式

$p(y|x;w,b) = y\cdot p_1(\hat x;\beta)+(1-y)\cdot p_0(\hat x;\beta)$

或者

$p(y|x;w,b) = [p_1(\hat x;\beta)]^y[p_0(\hat x;\beta)]^{1-y}$

根据对数似然函数的定义可知：
$$
lnL(w) = \sum_{i=1}^mlnf(y_i,w_1,w_2,\cdots,w_k)
$$
由于此时的y为离散型，所以将对数似然函数中的概率密度函数换成分布律即可
$$
l(w,b) =lnL(w) = \sum_{i=1}^mlnp(y_i|x_i;w,b)
$$
将$p(y|x;w,b) = y\cdot p_1(\hat x;\beta)+(1-y)\cdot p_0(\hat x;\beta)$代入对数似然函数可得：
$$
l(\beta) =\sum_{i=1}^mln( y_ip_1(\hat x_i;\beta)+(1-y_i)\cdot p_0(\hat x_i;\beta))
$$
由于$p_1(\hat w_i; \beta) =\frac{e^{\beta^Tx_i}}{1+e^{\beta^Tx_i}},p_0(\hat x_i;\beta)=\frac{1}{1+e^{\beta^Tx_i}}$

所以上式可以化为
$$
\begin{aligned}
l(\beta) &=\sum_{i=1}^m
ln(\frac{y_ie^{\beta^Tx_i}}{1+e^{\beta^Tx_i}}+
\frac{1-y_i}{1+e^{\beta^Tx_i}})
\\
&=\sum_{i=1}^m
ln(\frac{y_ie^{\beta^Tx_i}+1-y_i}{1+e^{\beta^Tx_i}})
\\
&=\sum_{i=1}^m
\bigg(ln(y_ie^{\beta^Tx_i}+1-y_i)-ln(1+e^{\beta^Tx_i})\bigg)
\end{aligned}
$$
由于$y_i \in \{0,1\}$所以

当$y_i =0$时
$$
\begin{aligned}
l(\beta)&=
\sum_{i=1}^m
\bigg(
ln(0\cdot e^{\beta^Tx_i}+1-0)
-
ln(1+e^{\beta^Tx_i})
\bigg)
\\
&=
\sum_{i=1}^m
\bigg(
ln1
-
ln(1+e^{\beta^Tx_i})
\bigg)
\\
&=
\sum_{i=1}^m
\bigg(
-
ln(1+e^{\beta^Tx_i})
\bigg)
\end{aligned}
$$
当$y_i =1$时
$$
\begin{aligned}
l(\beta)&=
\sum_{i=1}^m
\bigg(
ln(1\cdot e^{\beta^Tx_i}+1-1)
-
ln(1+e^{\beta^Tx_i})
\bigg)
\\
&=
\sum_{i=1}^m
\bigg(
lne^{\beta^Tx_i}
-
ln(1+e^{\beta^Tx_i})
\bigg)
\\
&=
\sum_{i=1}^m
\bigg(
\beta^Tx_i
-
ln(1+e^{\beta^Tx_i})
\bigg)
\end{aligned}
$$
综合可得
$$
l(\beta) =
\sum_{i=1}^m
\bigg(
y_i\beta^T\hat w_i
-
ln(1+e^{\beta^T \hat w_i})
\bigg)
$$
第二种极大似然估计法

若$p(y|x;w,b) = [p_1(\hat x;\beta)]^y[p_0(\hat x;\beta)]^{1-y}$将其带入对数似然函数可得
$$
\begin{aligned}
l(\beta)
&=
\sum_{i=1}^m
ln
(
[p_1(\hat x_i;\beta)]^y_i
[p_0(\hat x_i;\beta)]^{1-y_i}
)
\\
&=
\sum_{i=1}^m
\bigg[
ln[p_1(\hat x_i;\beta)]^y_i
+
ln[p_0(\hat x_i;\beta)]^{1-y_i}
\bigg]
\\
&=
\sum_{i=1}^m
\bigg[
y_iln[p_1(\hat x_i;\beta)]
+
(1-y_i)ln[p_0(\hat x_i;\beta)]
\bigg]

\\&=
\sum_{i=1}^m
\bigg[
y_i
\bigg[
ln[p_1(\hat x_i;\beta)]
-
ln[p_0(\hat x_i;\beta)]
\bigg]
+
ln[p_0(\hat x_i;\beta)]

\bigg]
\\
&=
\sum_{i=1}^m
\bigg[
y_i
ln
\big(
\frac
{p_1(\hat x_i;\beta)}
{p_0(\hat x_i;\beta)}
\big)
+
ln(p_0(\hat x_i;\beta))

\bigg]
\end{aligned}
$$
由于$p_1(\hat w_i; \beta) =\frac{e^{\beta^T\hat x_i}}{1+e^{\beta^T \hat x_i}},p_0(\hat x_i;\beta)=\frac{1}{1+e^{\beta^T \hat x_i}}$​

所以上式可以化为
$$
\begin{aligned}
l(\beta)
&=
\sum_{i=1}^m
\bigg[
y_i
ln
(
e^{\beta^T \hat x_i}
)
+
ln
\bigg(
\frac{1}{1+e^{\beta^T \hat x_i}}
\bigg)

\bigg]
\\
&=
\sum_{i=1}^m
\bigg[
y_i
\beta^T \hat x_i

-
ln
(1+e^{\beta^T \hat x_i})
\bigg]
\end{aligned}
$$
