# 什么是多项式回归

画一条曲线去拟合取值点。多项式（Polynomial）是代数学中的基础概念，是由称为未知数的变量和称为系数的常量通过有限次加法、加减法、乘法以及自然数幂次的乘方运算得到的代数表达式。多项式是整式的一种。未知数只有一个的多项式称为一元多项式；例如 $x^2-3x+4$ 就是一个一元多项式。未知数不止一个的多项式称为多元多项式，例如 $x^3-2xyz^2+2yz+1$ 就是一个三元多项式。

# 多项式回归可以干什么

预测股市的波动、交通流量等

# 多项式回归怎么推导的

多项式回归其实就是线性回归的曲线版

首先我们来实现一个二次多项式拟合

## 实现二次多项式拟合

### 代数

一元高阶多项式函数如下
$$
y(x,w) = w_0 + w_1x = w_2x^2 + ...+w_mx^m = \sum\limits_{j=0}^nw_jx^j \tag{1}
$$
使用上面的多项式去拟合散点时就是要确定两个要素：

多项式系数$w$

多项式阶数$m$

若我们手动指定m的大小，则就只要确定w的值即可，这就成了线性回归了
$$
y(x,w) = w_0+w_1x+w_2x^2 = \sum\limits_{j=0}^2w_jx^j
$$
求解$w_0,w_1,w_2$

### 线代

一个二项式的标准型为
$$
y(x, w) = w_0 + w_1x + w_2x^2
$$
我们可以把他化成线性回归

令$x = x_1,x^2 = x_2$

原方程就变成了

$y(x,w) = w_0+w_1x_1+w_2x_2$

即
$$
\left[\begin{array}{c}
{1,x_1.x_2}
\end{array}\right]
\left[\begin{array}{c}
{w_0\\w_1\\w_2}
\end{array}\right]
= y_1
$$
$y(x,w)=XW$

但是在计算一元高次或者是多元高次时特征矩阵的表达和计算过程就会非常复杂

# 多项式回归的使用

## 使用自定义函数进行拟合

### 二次多项式拟合

用一组数据进行演示

```python
# 加载示例数据
x = [4, 8, 12, 25, 32, 43, 58, 63, 69, 79]
y = [20, 33, 50, 56, 42, 31, 33, 46, 65, 75]
```



#### 实现函数

$$
 y(x,w) = w_0+w_1x+w_2x^2 = \sum\limits_{j=0}^2w_jx^j
$$

```python
def func(p, x):
    # 根据公式，定义 2 次多项式函数
    w0, w1, w2 = p
    f = w0 + w1*x + w2*x*x
    return f
```

#### 实现差值函数

```python
def err_func(p, x, y):
    # 残差函数（观测值与拟合值之间的差距）
    ret = func(p, x) - y
    return ret
```

#### 调用API，使用`Scipy`实现最小二乘法

```python
import numpy as np
from scipy.optimize import leastsq

p_init = np.random.randn(3)  # 生成 3 个随机数
# 使用 Scipy 提供的最小二乘法函数得到最佳拟合参数
parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))

print('Fitting Parameters: ', parameters[0])
```

我们这里得到的最佳拟合参数 $w_0$, $w_1$, $w_2$ 依次为 `3.76893117e+01`, `-2.60474147e-01` 和 `8.00078082e-03`。也就是说，我们拟合后的函数（保留两位有效数字）为：

$$ y(x) = 37 - 0.26x + 0.0080x^2 \tag{3} $$

#### 预测

```python
func(parameters[0], x_temp)
```

### 实现N次多项式拟合

全都借用函数来实现

```python
def fit_func(p, x):
    """根据公式，定义 n 次多项式函数
    """
    f = np.poly1d(p)
    return f(x)


def err_func(p, x, y):
    """残差函数（观测值与拟合值之间的差距）
    """
    ret = fit_func(p, x) - y
    return ret


def n_poly(n):
    """n 次多项式拟合
    """
    p_init = np.random.randn(n)  # 生成 n 个随机数
    parameters = leastsq(err_func, p_init, args=(np.array(x), np.array(y)))
    return parameters[0]
```

此时使用`n_poly(3)`的意思就是二次多项式拟合

**注意**这个的结果和顺序是高次到低次排列的

### 使用 scikit-learn 进行多项式拟合

除了像上面我们自己去定义多项式及实现多项式回归拟合过程，也可以使用 scikit-learn 提供的多项式回归方法来完成。这里，我们会用到`sklearn.preprocessing.PolynomialFeatures()` 这个类。`PolynomialFeatures()` 主要的作用是产生多项式特征矩阵。

```
sklearn.preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=True)
```

```
- degree: 多项式次数，默认为 2 次多项式
- interaction_only: 默认为 False，如果为 True 则产生相互影响的特征集。
- include_bias: 默认为 True，包含多项式中的截距项。
```

产生一个二次多项式对应的特征矩阵

```python
from sklearn.preprocessing import PolynomialFeatures

X = [2, -1, 3]
X_reshape = np.array(X).reshape(len(X), 1)  # 转换为列向量
# 使用 PolynomialFeatures 自动生成特征矩阵
PolynomialFeatures(degree=2, include_bias=False).fit_transform(X_reshape)
```

对于上方单元格中的矩阵，第 1 列为 $X^1$，第 2 列为 $X^2$。我们就可以通过多元线性方程 $ y(x, w) = w_0 + w_1x_1 + w_2x_2 $ 对数据进行拟合。

使用scikit-learn进行训练

```python
x = np.array(x).reshape(len(x), 1)  # 转换为列向量
y = np.array(y).reshape(len(y), 1)

# 使用 sklearn 得到 2 次多项式回归特征矩阵
poly_features = PolynomialFeatures(degree=2, include_bias=False)
poly_x = poly_features.fit_transform(x)

from sklearn.linear_model import LinearRegression

# 定义线性回归模型
model = LinearRegression()
model.fit(poly_x, y)  # 训练

# 得到模型拟合参数
model.intercept_, model.coef_
```

`model.intercept_`为截距，`model.coef_`为系数

# 实例演示

使用世界麻疹疫苗接种率数据集预测相应年份的麻疹疫苗接种率

## 导入数据集

```python
import pandas as pd 

df = pd.read_csv("vaccine.csv", header = 0)
```

本数据集由两列组成，year为年份，calues为世界麻疹疫苗接种率

## 画图看下趋势

```python
x = df['Year']
y = df['Values']

plt.plot(x, y , 'r') # 画折线图
plt.scatter(x, y) # 画散点图
```

## 划分数据集

```python
# 首先划分 dateframe 为训练集和测试集
train_df = df[:int(len(df)*0.7)]
test_df = df[int(len(df)*0.7):]
```

按$70\%和30\%$进行划分 

## 定义自变量和因变量

```python
# 定义训练和测试使用的自变量和因变量
X_train = train_df['Year'].values
y_train = train_df['Values'].values

X_test = test_df['Year'].values
y_test = test_df['Values'].values
```

## 训练线性回归模型

```python
from sklearn.linear_model import LinearRegression
# 建立线性回归模型
model = LinearRegression()
model.fit(X_train.reshape(len(X_train), 1), y_train.reshape(len(y_train), 1))
results = model.predict(X_test.reshape(len(X_test), 1))
results  # 线性回归模型在测试集上的预测结果
```

## 评价

```python
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error

print("线性回归平均绝对误差: ", mean_absolute_error(y_test, results.flatten()))
print("线性回归均方误差: ", mean_squared_error(y_test, results.flatten()))
```

## 训练2次多项式回归模型

### 2次多项式回归特征矩阵

```python
from sklearn.preprocessing import PolynomialFeatures
# 2 次多项式回归特征矩阵
poly_features_2 = PolynomialFeatures(degree=2, include_bias=False)
poly_X_train_2 = poly_features_2.fit_transform(X_train.reshape(len(X_train), 1))
poly_X_test_2 = poly_features_2.fit_transform(X_test.reshape(len(X_test), 1))
```

### 二次回归模型的训练与预测

```python
# 2 次多项式回归模型训练与预测
model = LinearRegression()
model.fit(poly_X_train_2, y_train.reshape(len(X_train), 1))  # 训练模型

results_2 = model.predict(poly_X_test_2)  # 预测结果

results_2.flatten()  # 打印扁平化后的预测结果
```

## 评价

```python
print("2 次多项式回归平均绝对误差: ", mean_absolute_error(y_test, results_2.flatten()))
print("2 次多项式均方误差: ", mean_squared_error(y_test, results_2.flatten()))
```

## 更高次多项式回归

通过实例化 `make_pipeline` 管道类，实现调用一次 `fit` 和 `predict` 方法即可应用于所有预测器。`make_pipeline` 是使用 sklearn 过程中的技巧创新，其可以将一个处理流程封装起来使用。

具体来讲，例如上面的多项式回归中，我们需要先使用 `PolynomialFeatures` 完成特征矩阵转换，再放入 `LinearRegression` 中。那么，`PolynomialFeatures + LinearRegression` 这一个处理流程，就可以通过 `make_pipeline` 封装起来使用。

```python
from sklearn.pipeline import make_pipeline

X_train = X_train.reshape(len(X_train), 1)
X_test = X_test.reshape(len(X_test), 1)
y_train = y_train.reshape(len(y_train), 1)

for m in [3, 4, 5]:
    model = make_pipeline(PolynomialFeatures(
        m, include_bias=False), LinearRegression())
    model.fit(X_train, y_train)
    pre_y = model.predict(X_test)
    print("{} 次多项式回归平均绝对误差: ".format(m),
          mean_absolute_error(y_test, pre_y.flatten()))
    print("{} 次多项式均方误差: ".format(m), mean_squared_error(y_test, pre_y.flatten()))
    print("---")
```

# 多项式回归次数的选择

根据误差指标MSE的图像来判断

```python
# 计算 m 次多项式回归预测结果的 MSE 评价指标并绘图
mse = []  # 用于存储各最高次多项式 MSE 值
m = 1  # 初始 m 值
m_max = 10  # 设定最高次数
while m <= m_max:
    model = make_pipeline(PolynomialFeatures(
        m, include_bias=False), LinearRegression())
    model.fit(X_train, y_train)  # 训练模型
    pre_y = model.predict(X_test)  # 测试模型
    mse.append(mean_squared_error(y_test, pre_y.flatten()))  # 计算 MSE
    m = m + 1

print("MSE 计算结果: ", mse)
# 绘图
plt.plot([i for i in range(1, m_max + 1)], mse, 'r')
plt.scatter([i for i in range(1, m_max + 1)], mse)

# 绘制图名称等
plt.title("MSE of m degree of polynomial regression")
plt.xlabel("m")
plt.ylabel("MSE")
```

